# æ–‡ä»¶è·¯å¾„: custom_train/train_conv2.py
import sys
import os

# 1. ç¡®ä¿èƒ½å¯¼å…¥å­æ¨¡å— (éå¸¸é‡è¦ï¼)
# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„ï¼Œå‘ä¸Šä¸¤çº§æ‰¾åˆ° external
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
# æ·»åŠ  SRigL æºç è·¯å¾„
sys.path.append(os.path.join(project_root, 'external', 'condensed-sparsity', 'src'))

import torch
import torch.nn as nn
import torch.optim as optim
from typing import List

# å¯¼å…¥ SRigL å’Œ MNIST æ¨¡å‹
from rigl_torch.rigl_constant_fan import RigLConstFanScheduler
from rigl_torch.models.mnist import MnistNet

# === å®šä¹‰è‡ªå®šä¹‰è°ƒåº¦å™¨ ===
class TargetLayerSRigLScheduler(RigLConstFanScheduler):
    def __init__(self, target_layer_name: str, *args, **kwargs):
        self.target_layer_name = target_layer_name
        super().__init__(*args, **kwargs)

    def _allocate_sparsity(self) -> List[float]:
        sparsity_dist = []
        found_target = False
        target_sparsity = 1.0 - self.dense_allocation

        for name in self.module_names:
            if name == self.target_layer_name:
                sparsity_dist.append(target_sparsity)
                found_target = True
                print(f"[SRigL] ğŸ¯ ç›®æ ‡å±‚é”å®š: '{name}', ç¨€ç–åº¦è®¾ç½®ä¸º {target_sparsity:.2f}")
            else:
                sparsity_dist.append(0.0) # å…¶ä»–å±‚ä¿æŒå¯†é›†
        
        if not found_target:
            raise ValueError(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚ '{self.target_layer_name}'")
        return sparsity_dist

# === ä¸»è®­ç»ƒé€»è¾‘ ===
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # åˆå§‹åŒ– LeNet5
    model = MnistNet().to(device)
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

    # åˆå§‹åŒ–è°ƒåº¦å™¨ï¼šåªè®­ç»ƒ conv2
    print("åˆå§‹åŒ–è°ƒåº¦å™¨...")
    scheduler = TargetLayerSRigLScheduler(
        target_layer_name="conv2", 
        model=model,
        optimizer=optimizer,
        dense_allocation=0.1,       # 90% ç¨€ç–
        T_end=2000,
        delta=100,
        alpha=0.3,
        static_topo=False,          # åŠ¨æ€è®­ç»ƒ
        ignore_linear_layers=False 
    )

    # æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒ (è¿™é‡Œç”¨éšæœºæ•°æ®æ¼”ç¤ºï¼Œä½ å¯ä»¥æ¢æˆçœŸå® DataLoader)
    print("å¼€å§‹è®­ç»ƒæ¼”ç¤º...")
    criterion = nn.CrossEntropyLoss()
    
    # æ¨¡æ‹Ÿ 500 æ­¥
    for step in range(500):
        # æ¨¡æ‹Ÿè¾“å…¥ (Batch=64, 1é€šé“, 28x28)
        data = torch.randn(64, 1, 28, 28).to(device)
        target = torch.randint(0, 10, (64,)).to(device)

        model.train()
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        
        optimizer.step() # SRigL åœ¨è¿™é‡Œç”Ÿæ•ˆ
        
        if scheduler(): 
            pass # æ‹“æ‰‘æ›´æ–°æ£€æŸ¥

        if step % 100 == 0:
            print(f"Step {step}: Loss {loss.item():.4f}")

    # éªŒè¯ç¨€ç–åº¦
    print("\n=== éªŒè¯ç¨€ç–åº¦ ===")
    for name, module in model.named_modules():
        if hasattr(module, "weight") and name in scheduler.module_names:
            w = module.weight.data
            sparsity = (w == 0).sum().item() / w.numel()
            mark = "âœ…" if sparsity > 0 else "Dense"
            print(f"å±‚: {name:10} | ç¨€ç–åº¦: {sparsity:.2%} {mark}")

if __name__ == "__main__":
    main()